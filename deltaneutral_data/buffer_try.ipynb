{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import io\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from fin_data_utils import FinDataUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_buffer_size = 10\n",
    "max_buffer_size = 1000\n",
    "\n",
    "my_queue = deque(maxlen=max_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaNeutralReader:\n",
    "    def __init__(self, credentials_file: str, folder_id: str, definitions_file: str, cache_dir: str = 'cache'):\n",
    "        #folder_id = \"google_drive_folder_id\"\n",
    "        #credentials_file = \"path_to_credentials_file.json\"\n",
    "        #definitions_file = \"path_to_definitions_file.json\"\n",
    "        #cache_dir = \"path_to_cache_directory\"\n",
    "\n",
    "        self.folder_id = folder_id\n",
    "        self.ivy_db = FinDataUtils(definitions_file)\n",
    "        \n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Set up Google Drive service\n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "            credentials_file, scopes=['https://www.googleapis.com/auth/drive.readonly']\n",
    "        )\n",
    "        self.service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "    def search_file(self, file_name: str) -> str:\n",
    "        query = f\"name='{file_name}' and '{self.folder_id}' in parents\"\n",
    "        results = self.service.files().list(\n",
    "            q=query, spaces='drive', fields='files(id, name, modifiedTime)'\n",
    "        ).execute()\n",
    "        items = results.get('files', [])\n",
    "        \n",
    "        if not items:\n",
    "            raise FileNotFoundError(f\"File {file_name} not found in the specified folder.\")\n",
    "        \n",
    "        return items[0]\n",
    "\n",
    "    def get_cache_path(self, file_name: str) -> Path:\n",
    "        return self.cache_dir / file_name\n",
    "\n",
    "    def download_and_cache(self, file_id: str, file_name: str) -> Path:\n",
    "        request = self.service.files().get_media(fileId=file_id)\n",
    "        file = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(file, request)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print(f\"Download {int(status.progress() * 100)}%.\")\n",
    "        \n",
    "        cache_path = self.get_cache_path(file_name)\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            f.write(file.getvalue())\n",
    "        \n",
    "        return cache_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily fname L3_options_20240415.csv\n",
      "monthly fname L3_2024_April.zip\n",
      "__contains__ checking for cache/L3_options_20240415.csv\n",
      "__contains__ checking for True\n"
     ]
    }
   ],
   "source": [
    "class cache():\n",
    "    def __init__(self, cache_dir):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def get_cache_path(self, file_name: str) -> Path:\n",
    "        return self.cache_dir / file_name\n",
    "    \n",
    "    \n",
    "    def is_cached(self,fname):\n",
    "        return (self.cache_dir / fname).exists()\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        print(f'__contains__ checking for {(self.cache_dir / key)}')\n",
    "        print(f'__contains__ checking for {(self.cache_dir / key).exists()}')\n",
    "        return (self.cache_dir / key).exists()\n",
    "    \n",
    "    def is_cache_valid(self, file_name: str, modified_time: str) -> bool:\n",
    "        cache_path = self.get_cache_path(file_name)\n",
    "        if not cache_path.exists():\n",
    "            return False\n",
    "        \n",
    "        cache_modified_time = cache_path.stat().st_mtime\n",
    "        drive_modified_time = pd.to_datetime(modified_time).timestamp()\n",
    "        \n",
    "        return cache_modified_time >= drive_modified_time\n",
    "    \n",
    "    def extract_zip(self, zip_file: Path):\n",
    "        with zipfile.ZipFile(zip_file) as zip_ref:\n",
    "            zip_ref.extractall(self.cache_dir)\n",
    "    \n",
    "    def file_exists_in_zip(self,zip_path, filename):\n",
    "        \"\"\"\n",
    "        Check if a file exists in a ZIP archive.\n",
    "\n",
    "        :param zip_path: Path to the ZIP archive.\n",
    "        :param filename: Name of the file to check for within the archive.\n",
    "        :return: True if the file exists in the archive, False otherwise.\n",
    "        \"\"\"\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            return filename in zip_ref.namelist()\n",
    "\n",
    "class ds_info():\n",
    "    def __init__(self, cache):\n",
    "        self.cache = cache\n",
    "    def get_archive_file_name(self,date)->str:\n",
    "        fname=f'L3_{date.strftime(\"%Y\")}_{date.strftime(\"%B\")}.zip'\n",
    "        return fname\n",
    "\n",
    "    def get_archive_file_name_daily(self,date)->str:\n",
    "        fname=f'L3_{date.strftime(\"%Y%m%d\")}.zip'\n",
    "        return fname\n",
    "\n",
    "    def get_daily_options_file_name(self,date)->str:\n",
    "        fname=f'L3_options_{date.strftime(\"%Y%m%d\")}.csv'\n",
    "        return fname\n",
    "\n",
    "\n",
    "\n",
    "my_date=pd.to_datetime(\"2024-04-15\")\n",
    "my_cache = cache(Path(\"cache\"))\n",
    "dsource=ds_info(my_cache)\n",
    "#get the filenames for daily and monthly files:\n",
    "verbose=True\n",
    "\n",
    "daily_fname=dsource.get_daily_options_file_name(my_date) \n",
    "monthly_fname=dsource.get_archive_file_name(my_date)\n",
    "if verbose:\n",
    "    print(f'daily fname {daily_fname}')\n",
    "    print(f'monthly fname {monthly_fname}')\n",
    "\n",
    "if not (daily_fname in my_cache):\n",
    "    print(f'not cached')\n",
    "\n",
    "    if monthly_fname in my_cache: \n",
    "        if my_cache.file_exists_in_zip(my_cache.get_cache_path(monthly_fname),daily_fname):\n",
    "            my_cache.extract_zip(my_cache.get_cache_path(monthly_fname))\n",
    "    else:\n",
    "        print(\"monthly not cached\") \n",
    "        #load from google drive\n",
    "\n",
    "    if my_cache.is_cached(daily_fname):\n",
    "        print(\"daily cached\")\n",
    "        with open(my_cache.get_cache_path(daily_fname)) as file:\n",
    "            df=pd.read_csv(file,header=None)\n",
    "            print(df.head())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick detour can I do better:\n",
    "# right now i have a cache class that returns the local name of the filename based on a date\n",
    "1) my_cache= cache(cache_dir=dir)\n",
    "2) if (daily_file(date) in cache)\n",
    "3)      fpath=cache(daily_file(date))\n",
    "\n",
    "# different way\n",
    "# try\n",
    "    try:\n",
    "        ds=load_local_daily(my_date)\n",
    "    catch:\n",
    "        if (not_in_cache):\n",
    "            try\n",
    "                ds=load_from_monthly_archive(my_date)\n",
    "            catch:\n",
    "                if (not_in_cache):\n",
    "                    ds=load_from_remote_drive(my_date)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_path(self, file_name: str) -> Path:\n",
    "    return self.cache_dir / file_name\n",
    "\n",
    "def is_cache_valid(self, file_name: str, modified_time: str) -> bool:\n",
    "    cache_path = self.get_cache_path(file_name)\n",
    "    if not cache_path.exists():\n",
    "        return False\n",
    "    \n",
    "    cache_modified_time = cache_path.stat().st_mtime\n",
    "    drive_modified_time = pd.to_datetime(modified_time).timestamp()\n",
    "    \n",
    "    return cache_modified_time >= drive_modified_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fin_data_utils\n",
    "from fin_data_utils import FinDataUtils\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import io\n",
    "\n",
    "class DeltaNeutralReader:\n",
    "    def __init__(self, credentials_file: str, folder_id: str, definitions_file: str, cache_dir: str = 'cache'):\n",
    "        #folder_id = \"google_drive_folder_id\"\n",
    "        #credentials_file = \"path_to_credentials_file.json\"\n",
    "        #definitions_file = \"path_to_definitions_file.json\"\n",
    "        #cache_dir = \"path_to_cache_directory\"\n",
    "\n",
    "        self.folder_id = folder_id\n",
    "        self.table_defs = FinDataUtils(definitions_file)\n",
    "        \n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Set up Google Drive service\n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "            credentials_file, scopes=['https://www.googleapis.com/auth/drive.readonly']\n",
    "        )\n",
    "        self.service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "    def search_file(self, file_name: str) -> str:\n",
    "        query = f\"name='{file_name}' and '{self.folder_id}' in parents\"\n",
    "        results = self.service.files().list(\n",
    "            q=query, spaces='drive', fields='files(id, name, modifiedTime)'\n",
    "        ).execute()\n",
    "        items = results.get('files', [])\n",
    "        \n",
    "        if not items:\n",
    "            raise FileNotFoundError(f\"File {file_name} not found in the specified folder.\")\n",
    "        \n",
    "        return items[0]\n",
    "\n",
    "    def get_cache_path(self, file_name: str) -> Path:\n",
    "        return self.cache_dir / file_name\n",
    "\n",
    "    def download_and_cache(self, file_id: str, file_name: str) -> Path:\n",
    "        request = self.service.files().get_media(fileId=file_id)\n",
    "        file = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(file, request)\n",
    "        done = False\n",
    "        while done is False:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print(f\"Download {int(status.progress() * 100)}%.\")\n",
    "        \n",
    "        cache_path = self.get_cache_path(file_name)\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            f.write(file.getvalue())\n",
    "        \n",
    "        return cache_path\n",
    "\n",
    "\n",
    "    def read_file(self, file_name: str, table_name: str) -> pd.DataFrame:\n",
    "        print (f'reading file {file_name}')\n",
    "        delimiter=',' #make delimiter a parameter\n",
    "        dtype_dict = self.table_defs.get_dtype_dict(table_name)\n",
    "        float_dtype_dict = {col: 'float64' if dtype.startswith('int') else dtype for col, dtype in dtype_dict.items()}\n",
    "\n",
    "        date_columns = [col['name'] for col in self.table_defs.get_table_structure(table_name) if col['type'] == 'date']\n",
    "        \n",
    "        df = pd.read_csv(\n",
    "            file_name,\n",
    "            delimiter=delimiter,\n",
    "            header=None,\n",
    "            names=self.table_defs.get_column_names(table_name),\n",
    "            dtype=float_dtype_dict,\n",
    "            parse_dates=date_columns,\n",
    "            na_values=['-99.99', 'nan', 'NaN', '']  # Add any other strings that represent NaN in your data\n",
    "        )\n",
    "\n",
    "        for col, dtype in dtype_dict.items():\n",
    "            if dtype.startswith('int'):\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n",
    "        \n",
    "        return df\n",
    "        \n",
    "        # Handle missing values (assuming -99.99 is used for missing values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fname final L3_options_20240415.csv\n",
      "tabe_defs=['Security', 'Security_Price', 'Option_Price', 'Option_Stats', 'Forward_Price']\n",
      "reading file cache/L3_options_20240415.csv\n",
      "  UnderlyingSymbol  InderlyingPrice Exchange      OptionSymbol  Type  \\\n",
      "0                A           140.22        *  A240419C00075000  call   \n",
      "1                A           140.22        *  A240419P00075000   put   \n",
      "2                A           140.22        *  A240419C00080000  call   \n",
      "3                A           140.22        *  A240419P00080000   put   \n",
      "4                A           140.22        *  A240419C00085000  call   \n",
      "\n",
      "  Expiration   DateDate  Strike  Last   Bid  ...  Open Interest  \\\n",
      "0 2024-04-19 2024-04-15    75.0  0.00  63.0  ...              0   \n",
      "1 2024-04-19 2024-04-15    75.0  0.05   0.0  ...             10   \n",
      "2 2024-04-19 2024-04-15    80.0  0.00  58.2  ...              0   \n",
      "3 2024-04-19 2024-04-15    80.0  0.00   0.0  ...              0   \n",
      "4 2024-04-19 2024-04-15    85.0  0.00  53.0  ...              0   \n",
      "\n",
      "   Open Interest2      IV  IV Bid  IV Ask  Delta  Gamma   Theta  Vega  \\\n",
      "0               0  0.3810     0.0  5.0789    1.0    0.0 -1.0199   0.0   \n",
      "1              10  0.4512     0.0  2.8423    0.0    0.0  0.0000   0.0   \n",
      "2               0  0.3810     0.0  4.6463    1.0    0.0 -1.0879   0.0   \n",
      "3               0  0.4512     0.0  2.5742    0.0    0.0  0.0000   0.0   \n",
      "4               0  0.3810     0.0  3.9606    1.0    0.0 -1.1559   0.0   \n",
      "\n",
      "              Alias  \n",
      "0  A240419C00075000  \n",
      "1  A240419P00075000  \n",
      "2  A240419C00080000  \n",
      "3  A240419P00080000  \n",
      "4  A240419C00085000  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "class FileNotInArchiveError(Exception):\n",
    "    \"\"\"A custom exception for specific error cases.\"\"\"\n",
    "    pass\n",
    "\n",
    "class ds_info():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def get_archive_file_name(date)->str:\n",
    "        fname=f'L3_{date.strftime(\"%Y\")}_{date.strftime(\"%B\")}.zip'\n",
    "        return fname\n",
    "\n",
    "    @staticmethod\n",
    "    def get_archive_file_name_daily(date)->str:\n",
    "        fname=f'L3_{date.strftime(\"%Y%m%d\")}.zip'\n",
    "        return fname\n",
    "\n",
    "    @staticmethod\n",
    "    def get_daily_options_file_name(date)->str:\n",
    "        fname=f'L3_options_{date.strftime(\"%Y%m%d\")}.csv'\n",
    "        return fname\n",
    "\n",
    "\n",
    "def load_daily_file_from_cache(fdate, cache_dir=\"cache\", **kwargs):\n",
    "    fname=ds_info.get_daily_options_file_name(fdate)\n",
    "    cache_path = Path(cache_dir) / fname\n",
    "    if not cache_path.exists():\n",
    "        raise FileNotFoundError(f\"File {fname} not found in the cache directory.\")\n",
    "    return fname\n",
    "\n",
    "def load_daily_file_from_monthly_cache(fdate, cache_dir=\"cache\", **kwargs):\n",
    "    fname=ds_info.get_archive_file_name(fdate)\n",
    "    daily_fname=ds_info.get_daily_options_file_name(fdate)\n",
    "    cache_path = Path(cache_dir) / fname\n",
    "    if not cache_path.exists():\n",
    "        raise FileNotFoundError(f\"File {fname} not found in the cache directory.\")\n",
    "\n",
    "    with zipfile.ZipFile(cache_path) as zip_ref:\n",
    "        if  (daily_fname in zip_ref.namelist()):\n",
    "            zip_ref.extractall(cache_dir)\n",
    "        else:\n",
    "            raise FileNotInArchiveError(f\"File {fname} not found in the cache directory.\")\n",
    "    return daily_fname\n",
    "\n",
    "def load_daily_file_from_remote_monthly(fdate, cache_dir=\"cache\", **kwargs):\n",
    "    print(f'load_daily_file_from_remote_monthly {fdate}')\n",
    "    fname=ds_info.get_archive_file_name(fdate)\n",
    "    remote_reader=kwargs['remote_reader']\n",
    "    file_info=remote_reader.search_file(fname)\n",
    "\n",
    "    print(f'file_info {file_info}')\n",
    "    daily_fname=remote_reader.download_and_cache(file_info['id'],fname)\n",
    "    load_daily_file_from_monthly_cache(fdate, cache_dir=cache_dir)\n",
    "    return daily_fname\n",
    "    #raise FileNotFoundError(f\"File {fdate} not found in the cache directory.\")\n",
    "\n",
    "def load_daily_file_from_remote_daily(fdate, cache_dir=\"cache\"):\n",
    "    pass\n",
    "\n",
    "load_functions=[\n",
    "    load_daily_file_from_cache,\n",
    "    load_daily_file_from_monthly_cache,\n",
    "    load_daily_file_from_remote_monthly\n",
    "]\n",
    "\n",
    "class data_file_cache():\n",
    "    def __init__(self,remote_reader,cache_dir=\"cache\"):\n",
    "        if not isinstance(cache_dir, Path):\n",
    "            cache_dir = Path(cache_dir)\n",
    "        self.cache_dir=cache_dir\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        self.remote_reader=remote_reader\n",
    "        self.load_functions=load_functions\n",
    "        # self.load_functions.append(self.load_daily_file_from_remote_monthly)\n",
    "\n",
    "    \"\"\"\n",
    "    def load_daily_file_from_remote_monthly(self,fdate, cache_dir=\"cache\"):\n",
    "        fname=ds_info.get_archive_file_name(fdate)\n",
    "        file_info=self.remote_reader.search_file(fname)\n",
    "        print(f'file_info {file_info}')\n",
    "        raise FileNotFoundError(f\"File {fdate} not found in the cache directory.\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_file(self,fdate):\n",
    "        success=False\n",
    "        fname=None\n",
    "        success=False\n",
    "        for load_func in self.load_functions:\n",
    "            try:\n",
    "                fname=load_func(fdate,self.cache_dir, remote_reader=self.remote_reader)\n",
    "                success=True\n",
    "                break\n",
    "            except(FileNotFoundError):\n",
    "                continue\n",
    "\n",
    "        return fname\n",
    "\n",
    "\n",
    "\n",
    "d_reader = DeltaNeutralReader(\n",
    "    credentials_file='testproject1-419520-61c1efd44a96.json',\n",
    "    folder_id='1jkJR0INuQLWSVMfaZjnMa2NkfJnt4w0q',\n",
    "    definitions_file='deltaneutral_table_definitions.json'\n",
    ")\n",
    "\n",
    "my_cache=data_file_cache(remote_reader=d_reader)\n",
    "fname=my_cache.load_file(pd.to_datetime(\"2024-04-15\"))\n",
    "print(f'fname final {fname}')\n",
    "#df=pd.read_csv(my_cache.cache_dir / fname)\n",
    "table_names=d_reader.table_defs.get_table_names()\n",
    "print(f'tabe_defs={table_names}')\n",
    "df=d_reader.read_file(my_cache.cache_dir / fname, 'Option_Price')\n",
    "\n",
    "#df = pd.read_csv(\n",
    "#                my_cache.cache_dir / fname,\n",
    "#                delimiter='\\t',\n",
    "#                header=None,\n",
    "#                names=d_reader.table_defs.get_column_names('options'),\n",
    "#                na_values=['-99.99', 'nan', 'NaN', '']  # Add any other strings that represent NaN in your data\n",
    "#            )\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "a=np.array([1,2,3,4,5])\n",
    "b=reduce(lambda x,y: x+y, a,0)\n",
    "b\n",
    "\n",
    "c=a.sum()\n",
    "d=0\n",
    "for i in a:\n",
    "    d+=i\n",
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
